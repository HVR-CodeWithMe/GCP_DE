steps:
  # Step 1: Deploy Cloud Composer (Updated to use a different region)
  - name: 'gcr.io/cloud-builders/gcloud'
    args: ['composer', 'environments', 'update', 'data-fetch-new', '--location', 'us-central1']  # Updated region

  # Step 2: Submit DataProc PySpark job (Updated region)
  - name: 'gcr.io/cloud-builders/gcloud'
    args:
      - 'dataproc'
      - 'jobs'
      - 'submit'
      - 'py'
      - '--region'
      - 'us-central1'  # Updated region
      - '--cluster'
      - 'cluster-0897'
      - '--py-files'
      - 'gs://temp-data-food/demo1.py,gs://temp-data-food/movie_data.py'
      - '--'
      - 'gs://temp-data-food/path/to/your-main-script.py'

options:
  logging: CLOUD_LOGGING_ONLY  # Logs will be sent to Cloud Logging
