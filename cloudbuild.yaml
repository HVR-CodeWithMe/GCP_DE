steps:
  # Step 1: Deploy Cloud Composer with required flags (e.g., node count)
  - name: 'gcr.io/cloud-builders/gcloud'
    args:
      - 'composer'
      - 'environments'
      - 'update'
      - 'data-fetch-new'
      - '--location'
      - 'asia-south2'  # Update this as per your requirement
      - '--node-count'
      - '3'  # Example: Update to 3 nodes

  # Step 2: Submit DataProc PySpark job
  - name: 'gcr.io/cloud-builders/gcloud'
    args:
      - 'dataproc'
      - 'jobs'
      - 'submit'
      - 'py'
      - '--region'
      - 'us-central1'
      - '--cluster'
      - 'cluster-0897'
      - '--py-files'
      - 'gs://temp-data-food/demo1.py,gs://temp-data-food/movie_data.py'
      - '--'
      - 'gs://temp-data-food/path/to/your-main-script.py'

options:
  logging: CLOUD_LOGGING_ONLY  # Logs will be sent to Cloud Logging
